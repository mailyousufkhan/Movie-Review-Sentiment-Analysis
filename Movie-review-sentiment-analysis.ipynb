{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "839fc1317e1b7253241839bbfa2d40303c53a3f1"
   },
   "source": [
    "## General information\n",
    "\n",
    "In this kernel I'll work with data from Movie Review Sentiment Analysis Playground Competition.\n",
    "\n",
    "This dataset is interesting for NLP researching. Sentences from original dataset were split in separate phrases and each of them has a sentiment label. Also a lot of phrases are really short which makes classifying them quite challenging. Let's try!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import datetime\n",
    "import lightgbm as lgb\n",
    "from scipy import stats\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "#from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "pd.set_option('max_colwidth',400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('C:/Users/Yousuf Khan/Data/movie-review-sentiment-analysis/train.tsv', sep=\"\\t\")\n",
    "test = pd.read_csv('C:/Users/Yousuf Khan/Data/movie-review-sentiment-analysis/test.tsv', sep=\"\\t\")\n",
    "sub = pd.read_csv('C:/Users/Yousuf Khan/Data/movie-review-sentiment-analysis/sampleSubmission.csv', sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "f9b8d8423bb09068cb168b67f4756ee8b250fc8c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage that what is good for the goose is also good for the gander , some of which occasionally amuses but none of which amounts to much of a story .</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage that what is good for the goose</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>A series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>of escapades demonstrating the adage that what is good for the goose</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>of</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>escapades demonstrating the adage that what is good for the goose</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>escapades</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>demonstrating the adage that what is good for the goose</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId  \\\n",
       "0         1           1   \n",
       "1         2           1   \n",
       "2         3           1   \n",
       "3         4           1   \n",
       "4         5           1   \n",
       "5         6           1   \n",
       "6         7           1   \n",
       "7         8           1   \n",
       "8         9           1   \n",
       "9        10           1   \n",
       "\n",
       "                                                                                                                                                                                         Phrase  \\\n",
       "0  A series of escapades demonstrating the adage that what is good for the goose is also good for the gander , some of which occasionally amuses but none of which amounts to much of a story .   \n",
       "1                                                                                                                 A series of escapades demonstrating the adage that what is good for the goose   \n",
       "2                                                                                                                                                                                      A series   \n",
       "3                                                                                                                                                                                             A   \n",
       "4                                                                                                                                                                                        series   \n",
       "5                                                                                                                          of escapades demonstrating the adage that what is good for the goose   \n",
       "6                                                                                                                                                                                            of   \n",
       "7                                                                                                                             escapades demonstrating the adage that what is good for the goose   \n",
       "8                                                                                                                                                                                     escapades   \n",
       "9                                                                                                                                       demonstrating the adage that what is good for the goose   \n",
       "\n",
       "   Sentiment  \n",
       "0          1  \n",
       "1          2  \n",
       "2          2  \n",
       "3          2  \n",
       "4          2  \n",
       "5          2  \n",
       "6          2  \n",
       "7          2  \n",
       "8          2  \n",
       "9          2  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "e94f8371d8be87186f23ecc75178480d3d96bd78"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>This quiet , introspective and entertaining independent is worth seeking .</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>65</td>\n",
       "      <td>2</td>\n",
       "      <td>This quiet , introspective and entertaining independent</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>66</td>\n",
       "      <td>2</td>\n",
       "      <td>This</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>67</td>\n",
       "      <td>2</td>\n",
       "      <td>quiet , introspective and entertaining independent</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>68</td>\n",
       "      <td>2</td>\n",
       "      <td>quiet , introspective and entertaining</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>69</td>\n",
       "      <td>2</td>\n",
       "      <td>quiet</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>70</td>\n",
       "      <td>2</td>\n",
       "      <td>, introspective and entertaining</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>71</td>\n",
       "      <td>2</td>\n",
       "      <td>introspective and entertaining</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>72</td>\n",
       "      <td>2</td>\n",
       "      <td>introspective and</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>73</td>\n",
       "      <td>2</td>\n",
       "      <td>introspective</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>74</td>\n",
       "      <td>2</td>\n",
       "      <td>and</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>75</td>\n",
       "      <td>2</td>\n",
       "      <td>entertaining</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>76</td>\n",
       "      <td>2</td>\n",
       "      <td>independent</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>77</td>\n",
       "      <td>2</td>\n",
       "      <td>is worth seeking .</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>78</td>\n",
       "      <td>2</td>\n",
       "      <td>is worth seeking</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>79</td>\n",
       "      <td>2</td>\n",
       "      <td>is worth</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>80</td>\n",
       "      <td>2</td>\n",
       "      <td>worth</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>81</td>\n",
       "      <td>2</td>\n",
       "      <td>seeking</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    PhraseId  SentenceId  \\\n",
       "63        64           2   \n",
       "64        65           2   \n",
       "65        66           2   \n",
       "66        67           2   \n",
       "67        68           2   \n",
       "68        69           2   \n",
       "69        70           2   \n",
       "70        71           2   \n",
       "71        72           2   \n",
       "72        73           2   \n",
       "73        74           2   \n",
       "74        75           2   \n",
       "75        76           2   \n",
       "76        77           2   \n",
       "77        78           2   \n",
       "78        79           2   \n",
       "79        80           2   \n",
       "80        81           2   \n",
       "\n",
       "                                                                        Phrase  \\\n",
       "63  This quiet , introspective and entertaining independent is worth seeking .   \n",
       "64                     This quiet , introspective and entertaining independent   \n",
       "65                                                                        This   \n",
       "66                          quiet , introspective and entertaining independent   \n",
       "67                                      quiet , introspective and entertaining   \n",
       "68                                                                       quiet   \n",
       "69                                            , introspective and entertaining   \n",
       "70                                              introspective and entertaining   \n",
       "71                                                           introspective and   \n",
       "72                                                               introspective   \n",
       "73                                                                         and   \n",
       "74                                                                entertaining   \n",
       "75                                                                 independent   \n",
       "76                                                          is worth seeking .   \n",
       "77                                                            is worth seeking   \n",
       "78                                                                    is worth   \n",
       "79                                                                       worth   \n",
       "80                                                                     seeking   \n",
       "\n",
       "    Sentiment  \n",
       "63          4  \n",
       "64          3  \n",
       "65          2  \n",
       "66          4  \n",
       "67          3  \n",
       "68          2  \n",
       "69          3  \n",
       "70          3  \n",
       "71          3  \n",
       "72          2  \n",
       "73          2  \n",
       "74          4  \n",
       "75          2  \n",
       "76          3  \n",
       "77          4  \n",
       "78          2  \n",
       "79          2  \n",
       "80          2  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.loc[train.SentenceId == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "4f75e24b86e3aeb7477fa1cd69789992233420b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average count of phrases per sentence in train is 18.\n",
      "Average count of phrases per sentence in test is 20.\n"
     ]
    }
   ],
   "source": [
    "print('Average count of phrases per sentence in train is {0:.0f}.'.format(train.groupby('SentenceId')['Phrase'].count().mean()))\n",
    "print('Average count of phrases per sentence in test is {0:.0f}.'.format(test.groupby('SentenceId')['Phrase'].count().mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "ca3148a6bbdd0f71e4909feb5dca874fa6d64a2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of phrases in train: 156060. Number of sentences in train: 8529.\n",
      "Number of phrases in test: 66292. Number of sentences in test: 3310.\n"
     ]
    }
   ],
   "source": [
    "print('Number of phrases in train: {}. Number of sentences in train: {}.'.format(train.shape[0], len(train.SentenceId.unique())))\n",
    "print('Number of phrases in test: {}. Number of sentences in test: {}.'.format(test.shape[0], len(test.SentenceId.unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "bef53d8f659b78e3fc6b1ed07025591d55b7c128"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average word length of phrases in train is 7.\n",
      "Average word length of phrases in test is 7.\n"
     ]
    }
   ],
   "source": [
    "print('Average word length of phrases in train is {0:.0f}.'.format(np.mean(train['Phrase'].apply(lambda x: len(x.split())))))\n",
    "print('Average word length of phrases in test is {0:.0f}.'.format(np.mean(test['Phrase'].apply(lambda x: len(x.split())))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9ee27697b41dfdca9cdb482bfc85cfd3d63ae6e2"
   },
   "source": [
    "We can see than sentences were split in 18-20 phrases at average and a lot of phrases contain each other. Sometimes one word or even one punctuation mark influences the sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9d1efbed65f250d37472544f4fe37cb6fd13e183",
    "collapsed": true
   },
   "source": [
    "Let's see for example most common trigrams for positive phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "4a128ab1d36eef6ec47161705a2b1094b830fe10"
   },
   "outputs": [],
   "source": [
    "text = ' '.join(train.loc[train.Sentiment == 4, 'Phrase'].values)\n",
    "text_trigrams = [i for i in ngrams(text.split(), 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "b5c8533c5861794d44b53dbb2e5ef764e5e88119"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('one', 'of', 'the'), 199),\n",
       " (('of', 'the', 'year'), 103),\n",
       " (('.', 'is', 'a'), 87),\n",
       " (('of', 'the', 'best'), 80),\n",
       " (('of', 'the', 'most'), 70),\n",
       " (('is', 'one', 'of'), 50),\n",
       " (('One', 'of', 'the'), 43),\n",
       " ((',', 'and', 'the'), 40),\n",
       " (('the', 'year', \"'s\"), 38),\n",
       " (('It', \"'s\", 'a'), 38),\n",
       " (('it', \"'s\", 'a'), 37),\n",
       " (('.', \"'s\", 'a'), 37),\n",
       " (('a', 'movie', 'that'), 35),\n",
       " (('the', 'edge', 'of'), 34),\n",
       " (('the', 'kind', 'of'), 33),\n",
       " (('of', 'your', 'seat'), 33),\n",
       " (('the', 'film', 'is'), 31),\n",
       " ((',', 'this', 'is'), 31),\n",
       " (('the', 'film', \"'s\"), 31),\n",
       " ((',', 'the', 'film'), 30),\n",
       " (('film', 'that', 'is'), 30),\n",
       " (('as', 'one', 'of'), 30),\n",
       " (('edge', 'of', 'your'), 29),\n",
       " ((',', 'it', \"'s\"), 27),\n",
       " (('a', 'film', 'that'), 27),\n",
       " (('as', 'well', 'as'), 27),\n",
       " ((',', 'funny', ','), 25),\n",
       " ((',', 'but', 'it'), 23),\n",
       " (('films', 'of', 'the'), 23),\n",
       " (('some', 'of', 'the'), 23)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(text_trigrams).most_common(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "402c0b58fe43a680bea33e1813012bec5c16cb55"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((',', 'funny', ','), 33),\n",
       " (('one', 'year', \"'s\"), 28),\n",
       " (('year', \"'s\", 'best'), 26),\n",
       " (('movies', 'ever', 'made'), 19),\n",
       " ((',', 'solid', 'cast'), 19),\n",
       " (('solid', 'cast', ','), 18),\n",
       " ((\"'ve\", 'ever', 'seen'), 16),\n",
       " (('.', 'It', \"'s\"), 16),\n",
       " ((',', 'making', 'one'), 15),\n",
       " (('best', 'films', 'year'), 15),\n",
       " ((',', 'touching', ','), 15),\n",
       " (('exquisite', 'acting', ','), 15),\n",
       " (('acting', ',', 'inventive'), 14),\n",
       " ((',', 'inventive', 'screenplay'), 14),\n",
       " (('jaw-dropping', 'action', 'sequences'), 14),\n",
       " (('good', 'acting', ','), 14),\n",
       " ((\"'s\", 'best', 'films'), 14),\n",
       " (('I', \"'ve\", 'seen'), 14),\n",
       " (('funny', ',', 'even'), 14),\n",
       " (('best', 'war', 'movies'), 13),\n",
       " (('purely', 'enjoyable', 'satisfying'), 13),\n",
       " (('funny', ',', 'touching'), 13),\n",
       " ((',', 'smart', ','), 13),\n",
       " (('inventive', 'screenplay', ','), 13),\n",
       " (('funniest', 'jokes', 'movie'), 13),\n",
       " (('action', 'sequences', ','), 13),\n",
       " (('sequences', ',', 'striking'), 13),\n",
       " ((',', 'striking', 'villains'), 13),\n",
       " (('exquisite', 'motion', 'picture'), 13),\n",
       " (('war', 'movies', 'ever'), 12)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = ' '.join(train.loc[train.Sentiment == 4, 'Phrase'].values)\n",
    "text = [i for i in text.split() if i not in stopwords.words('english')]\n",
    "text_trigrams = [i for i in ngrams(text, 3)]\n",
    "Counter(text_trigrams).most_common(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f59db6ed32fafb024728fd95b96157f278682a74"
   },
   "source": [
    "The results show the main problem with this dataset: there are to many common words due to sentenced splitted in phrases. As a result stopwords shouldn't be removed from text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "10dc8fc8d535ef492a3dab1b85b4052feec756ee"
   },
   "source": [
    "### Thoughts on feature processing and engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d5d018a694d10cbd9e8e89c26d5227fdb9cf8c0b"
   },
   "source": [
    "So, we have only phrases as data. And a phrase can contain a single word. And one punctuation mark can cause phrase to receive a different sentiment. Also assigned sentiments can be strange. This means several things:\n",
    "- using stopwords can be a bad idea, especially when phrases contain one single stopword;\n",
    "- puntuation could be important, so it should be used;\n",
    "- ngrams are necessary to get the most info from data;\n",
    "- using features like word count or sentence length won't be useful;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "bd603ad818970c3c8c6db5e430a6cb8ae8eafbd5"
   },
   "outputs": [],
   "source": [
    "tokenizer = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "7ebc937fd5ec811bc5c529ff416180fe338d073d"
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2), tokenizer=tokenizer.tokenize)\n",
    "full_text = list(train['Phrase'].values) + list(test['Phrase'].values)\n",
    "vectorizer.fit(full_text)\n",
    "train_vectorized = vectorizer.transform(train['Phrase'])\n",
    "test_vectorized = vectorizer.transform(test['Phrase'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "3d0bb4539b1e0b5f8439878a967ce7b5ece23f60"
   },
   "outputs": [],
   "source": [
    "y = train['Sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "75a7acfd815fb391cad92eed8d2f13e5c9801de8"
   },
   "outputs": [],
   "source": [
    "logreg = LogisticRegression()\n",
    "ovr = OneVsRestClassifier(logreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "5a0bf05b880e0d05da378b753394e5a631753e82"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yousuf Khan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 10.5 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False),\n",
       "          n_jobs=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "ovr.fit(train_vectorized, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "5946485410c25ae5033bf39e29f49f606ea87bfe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation mean accuracy 56.55%, std 0.07.\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(ovr, train_vectorized, y, scoring='accuracy', n_jobs=-1, cv=3)\n",
    "print('Cross-validation mean accuracy {0:.2f}%, std {1:.2f}.'.format(np.mean(scores) * 100, np.std(scores) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "755c2b04f7bf8e86e9dc6242f392a575c61a052c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation mean accuracy 56.51%, std 0.68.\n",
      "Wall time: 22.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "svc = LinearSVC(dual=False)\n",
    "scores = cross_val_score(svc, train_vectorized, y, scoring='accuracy', n_jobs=-1, cv=3)\n",
    "print('Cross-validation mean accuracy {0:.2f}%, std {1:.2f}.'.format(np.mean(scores) * 100, np.std(scores) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "55a5845de3412244cfa9f0a11392e50c76d78184"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yousuf Khan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "ovr.fit(train_vectorized, y);\n",
    "svc.fit(train_vectorized, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "398461363c7a395e2a982e07e8ac6fccaee139c1"
   },
   "source": [
    "## Deep learning\n",
    "And now let's try DL. DL should work better for text classification with multiple layers. I use an architecture similar to those which were used in toxic competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "eb29ec027df57f6597dbef976645dc8d151e1618"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D, GRU, CuDNNGRU, CuDNNLSTM, BatchNormalization\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D, MaxPooling1D, Add, Flatten\n",
    "from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\n",
    "from keras.models import Model, load_model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers, callbacks\n",
    "from keras import backend as K\n",
    "from keras.engine import InputSpec, Layer\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, Callback, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "a2881c29f82578b4a373b52d2c7b96a2e73bfd80"
   },
   "outputs": [],
   "source": [
    "tk = Tokenizer(lower = True, filters='')\n",
    "tk.fit_on_texts(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "1724669c7ca010d1bbf75e200211afdead768d1f"
   },
   "outputs": [],
   "source": [
    "train_tokenized = tk.texts_to_sequences(train['Phrase'])\n",
    "test_tokenized = tk.texts_to_sequences(test['Phrase'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "bcb80cf8a59ca779a0be1ab235a1e9da2f4b175b"
   },
   "outputs": [],
   "source": [
    "max_len = 50\n",
    "X_train = pad_sequences(train_tokenized, maxlen = max_len)\n",
    "X_test = pad_sequences(test_tokenized, maxlen = max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "9dfd0b8fa2c79bfa206d2fe8e35fbec444418f5c"
   },
   "outputs": [],
   "source": [
    "embedding_path = \"C:/Users/Yousuf Khan/Data/movie-review-sentiment-analysis/fasttext-crawl-300d-2m/crawl-300d-2M.vec\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "d74c2aa2f13e8544f045e5644d5bac70e248a8bd"
   },
   "outputs": [],
   "source": [
    "embed_size = 300\n",
    "max_features = 30000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_uuid": "cdb522c23b75331481145b789cf127b39d47eaa1"
   },
   "outputs": [],
   "source": [
    "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "embedding_index = dict(get_coefs(*o.strip().split(\" \")) for o in open(embedding_path, encoding=\"utf8\"))\n",
    "\n",
    "word_index = tk.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words + 1, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embedding_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_uuid": "365c0d607d55a78c5890268b9c168eb12a211855"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yousuf Khan\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe = OneHotEncoder(sparse=False)\n",
    "y_ohe = ohe.fit_transform(y.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_uuid": "594273c2d887315d083a35a5ffd7c2dd40c2ebb6"
   },
   "outputs": [],
   "source": [
    "def build_model1(lr=0.0, lr_d=0.0, units=0, spatial_dr=0.0, kernel_size1=3, kernel_size2=2, dense_units=128, dr=0.1, conv_size=32):\n",
    "    file_path = \"best_model.hdf5\"\n",
    "    check_point = ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1,\n",
    "                                  save_best_only = True, mode = \"min\")\n",
    "    early_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 3)\n",
    "    \n",
    "    inp = Input(shape = (max_len,))\n",
    "    x = Embedding(19479, embed_size, weights = [embedding_matrix], trainable = False)(inp)\n",
    "    x1 = SpatialDropout1D(spatial_dr)(x)\n",
    "\n",
    "    x_gru = Bidirectional(CuDNNGRU(units, return_sequences = True))(x1)\n",
    "    x1 = Conv1D(conv_size, kernel_size=kernel_size1, padding='valid', kernel_initializer='he_uniform')(x_gru)\n",
    "    avg_pool1_gru = GlobalAveragePooling1D()(x1)\n",
    "    max_pool1_gru = GlobalMaxPooling1D()(x1)\n",
    "    \n",
    "    x3 = Conv1D(conv_size, kernel_size=kernel_size2, padding='valid', kernel_initializer='he_uniform')(x_gru)\n",
    "    avg_pool3_gru = GlobalAveragePooling1D()(x3)\n",
    "    max_pool3_gru = GlobalMaxPooling1D()(x3)\n",
    "    \n",
    "    x_lstm = Bidirectional(CuDNNLSTM(units, return_sequences = True))(x1)\n",
    "    x1 = Conv1D(conv_size, kernel_size=kernel_size1, padding='valid', kernel_initializer='he_uniform')(x_lstm)\n",
    "    avg_pool1_lstm = GlobalAveragePooling1D()(x1)\n",
    "    max_pool1_lstm = GlobalMaxPooling1D()(x1)\n",
    "    \n",
    "    x3 = Conv1D(conv_size, kernel_size=kernel_size2, padding='valid', kernel_initializer='he_uniform')(x_lstm)\n",
    "    avg_pool3_lstm = GlobalAveragePooling1D()(x3)\n",
    "    max_pool3_lstm = GlobalMaxPooling1D()(x3)\n",
    "    \n",
    "    \n",
    "    x = concatenate([avg_pool1_gru, max_pool1_gru, avg_pool3_gru, max_pool3_gru,\n",
    "                    avg_pool1_lstm, max_pool1_lstm, avg_pool3_lstm, max_pool3_lstm])\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dr)(Dense(dense_units, activation='relu') (x))\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dr)(Dense(int(dense_units / 2), activation='relu') (x))\n",
    "    x = Dense(5, activation = \"sigmoid\")(x)\n",
    "    model = Model(inputs = inp, outputs = x)\n",
    "    model.compile(loss = \"binary_crossentropy\", optimizer = Adam(lr = lr, decay = lr_d), metrics = [\"accuracy\"])\n",
    "    history = model.fit(X_train, y_ohe, batch_size = 128, epochs = 20, validation_split=0.1, \n",
    "                        verbose = 1, callbacks = [check_point, early_stop])\n",
    "    model = load_model(file_path)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "95f52b1f6de4e939c8d21e3525503912282fbd47"
   },
   "source": [
    "An attempt at ensemble:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_uuid": "5eb586c98fb75c25cac099cd03d8233185fdc317"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 140454 samples, validate on 15606 samples\n",
      "Epoch 1/20\n",
      "140454/140454 [==============================] - 130s 924us/step - loss: 0.3597 - acc: 0.8378 - val_loss: 0.3219 - val_acc: 0.8526\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.32189, saving model to best_model.hdf5\n",
      "Epoch 2/20\n",
      "140454/140454 [==============================] - 102s 728us/step - loss: 0.3131 - acc: 0.8575 - val_loss: 0.3159 - val_acc: 0.8519\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.32189 to 0.31593, saving model to best_model.hdf5\n",
      "Epoch 3/20\n",
      "140454/140454 [==============================] - 102s 729us/step - loss: 0.3003 - acc: 0.8629 - val_loss: 0.3082 - val_acc: 0.8570\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.31593 to 0.30822, saving model to best_model.hdf5\n",
      "Epoch 4/20\n",
      "140454/140454 [==============================] - 102s 729us/step - loss: 0.2914 - acc: 0.8664 - val_loss: 0.3074 - val_acc: 0.8583\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.30822 to 0.30744, saving model to best_model.hdf5\n",
      "Epoch 5/20\n",
      "140454/140454 [==============================] - 102s 729us/step - loss: 0.2855 - acc: 0.8693 - val_loss: 0.3011 - val_acc: 0.8583\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.30744 to 0.30112, saving model to best_model.hdf5\n",
      "Epoch 6/20\n",
      "140454/140454 [==============================] - 103s 730us/step - loss: 0.2800 - acc: 0.8721 - val_loss: 0.3025 - val_acc: 0.8611\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.30112\n",
      "Epoch 7/20\n",
      "140454/140454 [==============================] - 103s 730us/step - loss: 0.2753 - acc: 0.8746 - val_loss: 0.3034 - val_acc: 0.8601\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.30112\n",
      "Epoch 8/20\n",
      "140454/140454 [==============================] - 103s 732us/step - loss: 0.2721 - acc: 0.8764 - val_loss: 0.3098 - val_acc: 0.8578\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.30112\n"
     ]
    }
   ],
   "source": [
    "model1 = build_model1(lr = 1e-3, lr_d = 1e-10, units = 64, spatial_dr = 0.3, dense_units=32, dr=0.1, conv_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_uuid": "b059392aad7d904adfb8ae151ad2004aa03da30d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 140454 samples, validate on 15606 samples\n",
      "Epoch 1/20\n",
      "140454/140454 [==============================] - 188s 1ms/step - loss: 0.3549 - acc: 0.8403 - val_loss: 0.3401 - val_acc: 0.8466\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.34011, saving model to best_model.hdf5\n",
      "Epoch 2/20\n",
      "140454/140454 [==============================] - 181s 1ms/step - loss: 0.3202 - acc: 0.8547 - val_loss: 0.3129 - val_acc: 0.8545\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.34011 to 0.31292, saving model to best_model.hdf5\n",
      "Epoch 3/20\n",
      "140454/140454 [==============================] - 181s 1ms/step - loss: 0.3100 - acc: 0.8588 - val_loss: 0.3086 - val_acc: 0.8563\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.31292 to 0.30858, saving model to best_model.hdf5\n",
      "Epoch 4/20\n",
      "140454/140454 [==============================] - 186s 1ms/step - loss: 0.3022 - acc: 0.8620 - val_loss: 0.3046 - val_acc: 0.8584\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.30858 to 0.30463, saving model to best_model.hdf5\n",
      "Epoch 5/20\n",
      "140454/140454 [==============================] - 191s 1ms/step - loss: 0.2959 - acc: 0.8649 - val_loss: 0.3054 - val_acc: 0.8575\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.30463\n",
      "Epoch 6/20\n",
      "140454/140454 [==============================] - 191s 1ms/step - loss: 0.2910 - acc: 0.8668 - val_loss: 0.3009 - val_acc: 0.8604\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.30463 to 0.30093, saving model to best_model.hdf5\n",
      "Epoch 7/20\n",
      "140454/140454 [==============================] - 205s 1ms/step - loss: 0.2862 - acc: 0.8694 - val_loss: 0.2989 - val_acc: 0.86062861 - acc: 0.8 - ETA: 2s - loss: 0.286\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.30093 to 0.29891, saving model to best_model.hdf5\n",
      "Epoch 8/20\n",
      "140454/140454 [==============================] - 196s 1ms/step - loss: 0.2830 - acc: 0.8710 - val_loss: 0.2992 - val_acc: 0.8611\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.29891\n",
      "Epoch 9/20\n",
      "140454/140454 [==============================] - 192s 1ms/step - loss: 0.2792 - acc: 0.8729 - val_loss: 0.2969 - val_acc: 0.8631\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.29891 to 0.29689, saving model to best_model.hdf5\n",
      "Epoch 10/20\n",
      "140454/140454 [==============================] - 199s 1ms/step - loss: 0.2761 - acc: 0.8750 - val_loss: 0.2980 - val_acc: 0.8620\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.29689\n",
      "Epoch 11/20\n",
      "140454/140454 [==============================] - 193s 1ms/step - loss: 0.2733 - acc: 0.8759 - val_loss: 0.3026 - val_acc: 0.8603\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.29689\n",
      "Epoch 12/20\n",
      "140454/140454 [==============================] - 180s 1ms/step - loss: 0.2716 - acc: 0.8771 - val_loss: 0.2990 - val_acc: 0.8618\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.29689\n"
     ]
    }
   ],
   "source": [
    "model2 = build_model1(lr = 1e-3, lr_d = 1e-10, units = 128, spatial_dr = 0.5, kernel_size1=3, kernel_size2=2, dense_units=64, dr=0.2, conv_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "_uuid": "8187e167ce93f0eb69f59cb9d7fedc4637a77cfe"
   },
   "outputs": [],
   "source": [
    "def build_model2(lr=0.0, lr_d=0.0, units=0, spatial_dr=0.0, kernel_size1=3, kernel_size2=2, dense_units=128, dr=0.1, conv_size=32):\n",
    "    file_path = \"best_model.hdf5\"\n",
    "    check_point = ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1,\n",
    "                                  save_best_only = True, mode = \"min\")\n",
    "    early_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 3)\n",
    "\n",
    "    inp = Input(shape = (max_len,))\n",
    "    x = Embedding(19479, embed_size, weights = [embedding_matrix], trainable = False)(inp)\n",
    "    x1 = SpatialDropout1D(spatial_dr)(x)\n",
    "\n",
    "    x_gru = Bidirectional(CuDNNGRU(units, return_sequences = True))(x1)\n",
    "    x_lstm = Bidirectional(CuDNNLSTM(units, return_sequences = True))(x1)\n",
    "    \n",
    "    x_conv1 = Conv1D(conv_size, kernel_size=kernel_size1, padding='valid', kernel_initializer='he_uniform')(x_gru)\n",
    "    avg_pool1_gru = GlobalAveragePooling1D()(x_conv1)\n",
    "    max_pool1_gru = GlobalMaxPooling1D()(x_conv1)\n",
    "    \n",
    "    x_conv2 = Conv1D(conv_size, kernel_size=kernel_size2, padding='valid', kernel_initializer='he_uniform')(x_gru)\n",
    "    avg_pool2_gru = GlobalAveragePooling1D()(x_conv2)\n",
    "    max_pool2_gru = GlobalMaxPooling1D()(x_conv2)\n",
    "    \n",
    "    \n",
    "    x_conv3 = Conv1D(conv_size, kernel_size=kernel_size1, padding='valid', kernel_initializer='he_uniform')(x_lstm)\n",
    "    avg_pool1_lstm = GlobalAveragePooling1D()(x_conv3)\n",
    "    max_pool1_lstm = GlobalMaxPooling1D()(x_conv3)\n",
    "    \n",
    "    x_conv4 = Conv1D(conv_size, kernel_size=kernel_size2, padding='valid', kernel_initializer='he_uniform')(x_lstm)\n",
    "    avg_pool2_lstm = GlobalAveragePooling1D()(x_conv4)\n",
    "    max_pool2_lstm = GlobalMaxPooling1D()(x_conv4)\n",
    "    \n",
    "    \n",
    "    x = concatenate([avg_pool1_gru, max_pool1_gru, avg_pool2_gru, max_pool2_gru,\n",
    "                    avg_pool1_lstm, max_pool1_lstm, avg_pool2_lstm, max_pool2_lstm])\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dr)(Dense(dense_units, activation='relu') (x))\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dr)(Dense(int(dense_units / 2), activation='relu') (x))\n",
    "    x = Dense(5, activation = \"sigmoid\")(x)\n",
    "    model = Model(inputs = inp, outputs = x)\n",
    "    model.compile(loss = \"binary_crossentropy\", optimizer = Adam(lr = lr, decay = lr_d), metrics = [\"accuracy\"])\n",
    "    history = model.fit(X_train, y_ohe, batch_size = 128, epochs = 20, validation_split=0.1, \n",
    "                        verbose = 1, callbacks = [check_point, early_stop])\n",
    "    model = load_model(file_path)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "_uuid": "9bf90d6d4effebb3c5aa9b666b8e09c9c57d94d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 140454 samples, validate on 15606 samples\n",
      "Epoch 1/20\n",
      "140454/140454 [==============================] - 120s 856us/step - loss: 0.4754 - acc: 0.7719 - val_loss: 0.3747 - val_acc: 0.8342\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.37475, saving model to best_model.hdf5\n",
      "Epoch 2/20\n",
      "140454/140454 [==============================] - 117s 830us/step - loss: 0.3712 - acc: 0.8353 - val_loss: 0.3379 - val_acc: 0.8468\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.37475 to 0.33790, saving model to best_model.hdf5\n",
      "Epoch 3/20\n",
      "140454/140454 [==============================] - 118s 842us/step - loss: 0.3501 - acc: 0.8444 - val_loss: 0.3292 - val_acc: 0.8491\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.33790 to 0.32916, saving model to best_model.hdf5\n",
      "Epoch 4/20\n",
      "140454/140454 [==============================] - 118s 839us/step - loss: 0.3404 - acc: 0.8468 - val_loss: 0.3235 - val_acc: 0.8498\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.32916 to 0.32352, saving model to best_model.hdf5\n",
      "Epoch 5/20\n",
      "140454/140454 [==============================] - 118s 838us/step - loss: 0.3347 - acc: 0.8492 - val_loss: 0.3206 - val_acc: 0.8508\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.32352 to 0.32061, saving model to best_model.hdf5\n",
      "Epoch 6/20\n",
      "140454/140454 [==============================] - 116s 825us/step - loss: 0.3296 - acc: 0.8507 - val_loss: 0.3198 - val_acc: 0.8514\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.32061 to 0.31975, saving model to best_model.hdf5\n",
      "Epoch 7/20\n",
      "140454/140454 [==============================] - 116s 825us/step - loss: 0.3261 - acc: 0.8519 - val_loss: 0.3170 - val_acc: 0.8524\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.31975 to 0.31696, saving model to best_model.hdf5\n",
      "Epoch 8/20\n",
      "140454/140454 [==============================] - 116s 825us/step - loss: 0.3224 - acc: 0.8534 - val_loss: 0.3137 - val_acc: 0.8533\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.31696 to 0.31371, saving model to best_model.hdf5\n",
      "Epoch 9/20\n",
      "140454/140454 [==============================] - 116s 824us/step - loss: 0.3200 - acc: 0.8542 - val_loss: 0.3134 - val_acc: 0.8539\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.31371 to 0.31337, saving model to best_model.hdf5\n",
      "Epoch 10/20\n",
      "140454/140454 [==============================] - 116s 824us/step - loss: 0.3176 - acc: 0.8554 - val_loss: 0.3137 - val_acc: 0.8549\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.31337\n",
      "Epoch 11/20\n",
      "140454/140454 [==============================] - 116s 828us/step - loss: 0.3154 - acc: 0.8566 - val_loss: 0.3161 - val_acc: 0.8533\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.31337\n",
      "Epoch 12/20\n",
      "140454/140454 [==============================] - 117s 832us/step - loss: 0.3131 - acc: 0.8571 - val_loss: 0.3107 - val_acc: 0.8552\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.31337 to 0.31074, saving model to best_model.hdf5\n",
      "Epoch 13/20\n",
      "140454/140454 [==============================] - 117s 831us/step - loss: 0.3108 - acc: 0.8582 - val_loss: 0.3113 - val_acc: 0.8558\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.31074\n",
      "Epoch 14/20\n",
      "140454/140454 [==============================] - 116s 825us/step - loss: 0.3093 - acc: 0.8587 - val_loss: 0.3116 - val_acc: 0.8549\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.31074\n",
      "Epoch 15/20\n",
      "140454/140454 [==============================] - 116s 823us/step - loss: 0.3070 - acc: 0.8597 - val_loss: 0.3120 - val_acc: 0.8560\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.31074\n"
     ]
    }
   ],
   "source": [
    "model3 = build_model2(lr = 1e-4, lr_d = 0, units = 64, spatial_dr = 0.5, kernel_size1=4, kernel_size2=3, dense_units=32, dr=0.1, conv_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "_uuid": "bf6d8d367c5adc30e00bbd77c1de70fd52960441"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 140454 samples, validate on 15606 samples\n",
      "Epoch 1/20\n",
      "140454/140454 [==============================] - 120s 851us/step - loss: 0.3745 - acc: 0.8323 - val_loss: 0.3227 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.32266, saving model to best_model.hdf5\n",
      "Epoch 2/20\n",
      "140454/140454 [==============================] - 115s 820us/step - loss: 0.3263 - acc: 0.8532 - val_loss: 0.3127 - val_acc: 0.8546\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.32266 to 0.31270, saving model to best_model.hdf5\n",
      "Epoch 3/20\n",
      "140454/140454 [==============================] - 115s 821us/step - loss: 0.3159 - acc: 0.8564 - val_loss: 0.3108 - val_acc: 0.8561\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.31270 to 0.31076, saving model to best_model.hdf5\n",
      "Epoch 4/20\n",
      "140454/140454 [==============================] - 115s 820us/step - loss: 0.3081 - acc: 0.8595 - val_loss: 0.3035 - val_acc: 0.8594\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.31076 to 0.30353, saving model to best_model.hdf5\n",
      "Epoch 5/20\n",
      "140454/140454 [==============================] - 115s 821us/step - loss: 0.3009 - acc: 0.8625 - val_loss: 0.3062 - val_acc: 0.8582\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.30353\n",
      "Epoch 6/20\n",
      "140454/140454 [==============================] - 115s 820us/step - loss: 0.2966 - acc: 0.8644 - val_loss: 0.3108 - val_acc: 0.8576\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.30353\n",
      "Epoch 7/20\n",
      "140454/140454 [==============================] - 115s 821us/step - loss: 0.2917 - acc: 0.8670 - val_loss: 0.3031 - val_acc: 0.8600\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.30353 to 0.30310, saving model to best_model.hdf5\n",
      "Epoch 8/20\n",
      "140454/140454 [==============================] - 115s 821us/step - loss: 0.2875 - acc: 0.8686 - val_loss: 0.3012 - val_acc: 0.8618\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.30310 to 0.30119, saving model to best_model.hdf5\n",
      "Epoch 9/20\n",
      "140454/140454 [==============================] - 115s 819us/step - loss: 0.2845 - acc: 0.8698 - val_loss: 0.3014 - val_acc: 0.8610\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.30119\n",
      "Epoch 10/20\n",
      "140454/140454 [==============================] - 115s 821us/step - loss: 0.2819 - acc: 0.8711 - val_loss: 0.3001 - val_acc: 0.8619\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.30119 to 0.30009, saving model to best_model.hdf5\n",
      "Epoch 11/20\n",
      "140454/140454 [==============================] - 115s 820us/step - loss: 0.2793 - acc: 0.8729 - val_loss: 0.3007 - val_acc: 0.8608\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.30009\n",
      "Epoch 12/20\n",
      "140454/140454 [==============================] - 115s 822us/step - loss: 0.2772 - acc: 0.8734 - val_loss: 0.3058 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.30009\n",
      "Epoch 13/20\n",
      "140454/140454 [==============================] - 115s 821us/step - loss: 0.2751 - acc: 0.8750 - val_loss: 0.3027 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.30009\n"
     ]
    }
   ],
   "source": [
    "model4 = build_model2(lr = 1e-3, lr_d = 0, units = 64, spatial_dr = 0.5, kernel_size1=3, kernel_size2=3, dense_units=64, dr=0.3, conv_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "_uuid": "b10113439be683bf19930750eaf96328e5b58d42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 140454 samples, validate on 15606 samples\n",
      "Epoch 1/20\n",
      "140454/140454 [==============================] - 133s 949us/step - loss: 0.3724 - acc: 0.8327 - val_loss: 0.3536 - val_acc: 0.8381\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.35361, saving model to best_model.hdf5\n",
      "Epoch 2/20\n",
      "140454/140454 [==============================] - 128s 909us/step - loss: 0.3211 - acc: 0.8544 - val_loss: 0.3145 - val_acc: 0.8548\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.35361 to 0.31445, saving model to best_model.hdf5\n",
      "Epoch 3/20\n",
      "140454/140454 [==============================] - 128s 908us/step - loss: 0.3086 - acc: 0.8592 - val_loss: 0.3082 - val_acc: 0.8562\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.31445 to 0.30824, saving model to best_model.hdf5\n",
      "Epoch 4/20\n",
      "140454/140454 [==============================] - 127s 908us/step - loss: 0.2987 - acc: 0.8628 - val_loss: 0.3050 - val_acc: 0.8603\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.30824 to 0.30499, saving model to best_model.hdf5\n",
      "Epoch 5/20\n",
      "140454/140454 [==============================] - 128s 909us/step - loss: 0.2910 - acc: 0.8665 - val_loss: 0.3035 - val_acc: 0.8599\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.30499 to 0.30351, saving model to best_model.hdf5\n",
      "Epoch 6/20\n",
      "140454/140454 [==============================] - 128s 914us/step - loss: 0.2847 - acc: 0.8694 - val_loss: 0.3104 - val_acc: 0.8572\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.30351\n",
      "Epoch 7/20\n",
      "140454/140454 [==============================] - 128s 911us/step - loss: 0.2789 - acc: 0.8725 - val_loss: 0.3021 - val_acc: 0.8623\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.30351 to 0.30205, saving model to best_model.hdf5\n",
      "Epoch 8/20\n",
      "140454/140454 [==============================] - 128s 914us/step - loss: 0.2743 - acc: 0.8752 - val_loss: 0.3052 - val_acc: 0.8604\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.30205\n",
      "Epoch 9/20\n",
      "140454/140454 [==============================] - 128s 913us/step - loss: 0.2700 - acc: 0.8772 - val_loss: 0.3110 - val_acc: 0.8621\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.30205\n",
      "Epoch 10/20\n",
      "140454/140454 [==============================] - 129s 915us/step - loss: 0.2667 - acc: 0.8792 - val_loss: 0.3086 - val_acc: 0.8586\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.30205\n"
     ]
    }
   ],
   "source": [
    "model5 = build_model2(lr = 1e-3, lr_d = 1e-7, units = 64, spatial_dr = 0.3, kernel_size1=3, kernel_size2=3, dense_units=64, dr=0.4, conv_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "_uuid": "8529014d1a239f308ac5f2552088ce2d8ebb8966"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66292/66292 [==============================] - 16s 236us/step\n",
      "66292/66292 [==============================] - 26s 385us/step\n",
      "66292/66292 [==============================] - 16s 248us/step\n",
      "66292/66292 [==============================] - 16s 247us/step\n",
      "66292/66292 [==============================] - 18s 265us/step\n"
     ]
    }
   ],
   "source": [
    "pred1 = model1.predict(X_test, batch_size = 1024, verbose = 1)\n",
    "pred = pred1\n",
    "pred2 = model2.predict(X_test, batch_size = 1024, verbose = 1)\n",
    "pred += pred2\n",
    "pred3 = model3.predict(X_test, batch_size = 1024, verbose = 1)\n",
    "pred += pred3\n",
    "pred4 = model4.predict(X_test, batch_size = 1024, verbose = 1)\n",
    "pred += pred4\n",
    "pred5 = model5.predict(X_test, batch_size = 1024, verbose = 1)\n",
    "pred += pred5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "_uuid": "d58a5b52ea647dab51123ef89878c5355b3d2971"
   },
   "outputs": [],
   "source": [
    "predictions = np.round(np.argmax(pred, axis=1)).astype(int)\n",
    "sub['Sentiment'] = predictions\n",
    "sub.to_csv(\"C:/Users/Yousuf Khan/Data/movie-review-sentiment-analysis/blend.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
